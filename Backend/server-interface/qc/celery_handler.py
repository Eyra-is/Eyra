import redis
import datetime
import time

#: Relative imports
from celery import Celery
from . import celery_config

# this code is generated by setupActiveModules.py script
# @@CELERYQCBASETASKIMPORTS
from .modules.TestModule import TestTask
# @@/CELERYQCBASETASKIMPORTS

host = celery_config.const['host']
port = celery_config.const['port']
broker_db = celery_config.const['broker_db']
backend_db = celery_config.const['backend_db']

broker = 'redis://{}:{}/{}'.format(host, port, broker_db)

celery = Celery(broker=broker)
celery.conf.update(
    CELERY_RESULT_BACKEND='redis://{}:{}/{}'.format(host, port, backend_db)
)

# connect to redis
_redis = redis.StrictRedis(host=host, port=port, db=backend_db)

def isSessionOver(sessionId) -> bool:
    """
    Returns True if session(Id) is over. 

    Finds out by using celery_config.const['session_timeout'] and checking
    if the time since client last queried this session is more than said
    timeout. If so, the session is over.
    """
    try:
        prevTime = datetime.datetime.strptime(
            _redis.get('session/{}/timestamp'.format(sessionId)).decode('utf-8'), 
            "%Y-%m-%d %H:%M:%S.%f")
    except AttributeError:
        # TODO Log error
        # redis get is None
        raise
    except ValueError:
        # TODO Log error
        # datetime parse failed
        raise

    if (datetime.datetime.now() - prevTime).seconds\
        > celery_config.const['session_timeout']:
        return True
    return False


# this code is used as template for the setupActiveModules.py script
# should be commented out here
# @@CELERYQCPROCESSTEMPLATE
# @celery.task(base=DummyTask, name='celery.qcProcSessionDummyModule')
# def qcProcSessionDummyModule(name, sessionId, prevTime=None, slistIdx=0, batchSize=5) -> None:
#     """
#     Goes through the list of recordings for this session in the redis database
#     located at: 'session/sessionId/recordings', containing a list of all current
#     recordings of this session, in the backend continuing from
#     slistIdx. 

#     Performs qcProcSessionDummyModule.processBatch which does the processing, 
#     it must take exactly 3 arguments, the first being the name used for identification
#     in the redis datastore (e.g. 'report/name/sessionId'), and
#     the second is sessionId, third is a list of indices of recordings to process. 
#     processBatch is responsible for putting the results on the correct format in the
#     report in the redis datastore. Obviously, processBatch needs to be a
#     synchronous function.

#     prevTime is the timestamp at which the previous instance of this task
#     put itself back on the queue. prevTime should be None when this chain
#     is started for the first time. Checks are made if the this prevTime was too short a while
#     ago to continue just adding tasks in rapid succession on the queue which takes up CPU,
#     in which case a delay is made.

#     Only processes batchSize recs at a time, until calling itself recursively
#     with the updated slistIdx (and by that placing itself at the back
#     of the celery queue), look at instagram, that's how they do it xD
#     """

#     if isSessionOver(sessionId):
#         return

#     recsInfo = _redis.get('session/{}/recordings'.format(sessionId))
#     if recsInfo is not None:
#         recsInfo = json.loads(recsInfo.decode('utf-8'))

#     curTime = datetime.datetime.now()
#     if prevTime is not None:
#         diff = (curTime - prevTime).microseconds
#         if diff < celery_config.const['task_min_proc_time']:
#             # the last processing task was issued only less than task_min_proc_time ago
#             #   wait for a second, in order to not put tasks in such rapid succession
#             #   on the queue.
#             time.sleep(celery_config.const['task_delay'])

#     result = qcProcSessionDummyModule.processBatch(name, sessionId, list(range(slistIdx, slistIdx+batchSize)))
#     if result:
#        qcProcSessionDummyModule.apply_async(
#            args=[name, sessionId, curTime, slistIdx+batchSize, batchSize])
# @@/CELERYQCPROCESSTEMPLATE

# this code is generated by setupActiveModules.py script
# @@CELERYQCPROCESSTASKS
@celery.task(base=DummyTask, name='celery.qcProcSessionDummyModule')
def qcProcSessionDummyModule(name, sessionId, prevTime=None, slistIdx=0, batchSize=5) -> None:
    """
    Goes through the list of recordings for this session in the redis database
    located at: 'session/sessionId/recordings', containing a list of all current
    recordings of this session, in the backend continuing from
    slistIdx. 

    Performs qcProcSessionDummyModule.processBatch which does the processing, 
    it must take exactly 3 arguments, the first being the name used for identification
    in the redis datastore (e.g. 'report/name/sessionId'), and
    the second is sessionId, third is a list of indices of recordings to process. 
    processBatch is responsible for putting the results on the correct format in the
    report in the redis datastore. Obviously, processBatch needs to be a
    synchronous function.

    prevTime is the timestamp at which the previous instance of this task
    put itself back on the queue. prevTime should be None when this chain
    is started for the first time. Checks are made if the this prevTime was too short a while
    ago to continue just adding tasks in rapid succession on the queue which takes up CPU,
    in which case a delay is made.

    Only processes batchSize recs at a time, until calling itself recursively
    with the updated slistIdx (and by that placing itself at the back
    of the celery queue), look at instagram, that's how they do it xD
    """

    if isSessionOver(sessionId):
        return

    recsInfo = _redis.get('session/{}/recordings'.format(sessionId))
    if recsInfo is not None:
        recsInfo = json.loads(recsInfo.decode('utf-8'))

    curTime = datetime.datetime.now()
    if prevTime is not None:
        diff = (curTime - prevTime).microseconds
        if diff < celery_config.const['task_min_proc_time']:
            # the last processing task was issued only less than task_min_proc_time ago
            #   wait for a second, in order to not put tasks in such rapid succession
            #   on the queue.
            time.sleep(celery_config.const['task_delay'])

    result = qcProcSessionDummyModule.processBatch(name, sessionId, list(range(slistIdx, slistIdx+batchSize)))
    if result:
       qcProcSessionDummyModule.apply_async(
           args=[name, sessionId, curTime, slistIdx+batchSize, batchSize])


# @@/CELERYQCPROCESSTASKS